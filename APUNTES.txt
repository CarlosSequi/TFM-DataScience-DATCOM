Literatura generica de monotonicos:
	-Kotlowski:
		-funcion monotona: una funcion es monotona si y solo si x<=y implica f(x)<=f(y)
		-clasificacion ordinal con restricciones monotonicas: maneja conocimiento subyacente del
		 problema sobre clases ordenadas, atributos ordenados y una relacion monotonica entre
		 la evaluacion de los atributos de una instancia y la asignacion de esta a una clase.
		-principio de dominancia: a mayor valor en atributos de una instancia, mayor sera el
		 valor de la clase a la que se asigna dicha instancia (clasificacion ordinal con
		 restricciones monotonicas)
		-si no hay relacion de monotonia en la asociacion de una clase a una instancia, 
		 pero las clases si poseen un orden se considera clasificacion ordinal tan solo
		-con las restricciones de monotonia presentes se puede trabajar con una amplia
		 variedad de funciones sin temor a que introduzcan más restricciones que 
		 la de monotonia: es posible hacer inferencia de la clase sobre todas las funciones
		 monotonas.
		-proposito del paper:
			-formalizar la aproximacion de clasificacion con restricciones monotonicas
			-analizar los metodos no parametricos de clasificacion
				-plug-in: estimando la distribucion condicional de la clase. Provienen de la
				 clasificacion isotonica (monotona creciente o decreciente)
				-empirical risk (direct approach): minimizando el riesgo empirico
		-usando el termino de "relacion de dominancia" decimos que una instancia x domina a
		 otra instancia x' cuando cada una de las variables de entrada de x (atributos de x)
		 son mayores o iguales que cada uno de los de x', se denota x>=x' y por tanto
		 x tendra asignada una etiqueta de clase mayor que x'.
		-una funcion es monotona si x>=x' -> h(x)>=h(x'). Es decir, si x domina a x', la
		 inferencia de clase de x ha de ser superior a la de x'.
		-RESTRICCIONES MONOTONICAS:
			-seran restricciones con respecto a la probabilidad de distribución de los 
			 datos y con repecto a las imposiciones de la función de perdida bajo las cuales
			 el clasificador optimo de Bayes es monotono.
			-DOMINANCIA ESTOCASTICA:
				-la restriccion de monotonia comentada (si  x>=x' -> h(x)>=h(x')) no siempre
				 se aplica en la practica de forma tan restrictiva, por ello hemos de 
				 hablar en términos probabilisticos a la hora de referirnos a dichas
				 restricciones
				-Decimos entonces que, siendo 'k' una de las posibles clases a tomar en el 
				 dominio por una instancia 'x', y siendo 'y' la etiqueta asignada a una
				 instancia 'x', si la restricción monótona nos dice que x>=x', entonces
				 la dominancia estocastica nos dice que P(y<=k|x)<=P(y<=k|x').
				 Es decir, que la probabilidad de que el valor asignado (y) a la instancia 
				 dominante (x) sea mayor que cierto valor de la clase fijado (k), es mayor
				 que la probabilidad de que el valor asignado (y) a la instancia dominada(x')
				 sea mayor que ese mismo cierto valor de la clase fijado.
				-la relacion de dominancia estocastica entre distribuciones se denota asi:
					x >= x' -> P(y|x) >= P(y|x')
			-CLASIFICADOR MONOTONO DE BAYES
				-en el problema de clasificacion el objetivo es encontrar el clasificador más
				 parecido al clasificador de Bayes, es decir, esta es nuestra función objetivo.
				-Siendo esta nuestra funcion objetivo, es un requisito que tambien aplique
				 las restricciones de monotonía que hemos enunciado. Problema: aunque la
				 distribucion de probabilidad tiene restricciones monotonicas, el clasificador
				 de Bayes no siempre las mantiene.
				-para solucionar este problema y mantener la monotonia en el clasificador de 
				 Bayes, han de imponerse las siguientes restricciones a la funcion de perdida
				 (L):
				 	-L(y,k+1)-L(y,k) >= L(y+1,k+1)-L(y+1,k)
					 	-demostrado en  Computer Society Digital Library at 
					 	http://doi.ieeecomputersociety.org/10.1109/TKDE.2012.204
					 	-esta caracteristica de la funcion de perdida es necesaria
					 	 en la clasificacion con restricciones monotonicas, si no no
					 	  tendria sentido minimizar el riesgo dentro de la clase de las funciones monotonas.
					-la siguiente definicion de convexidad es necesaria tambien para
					 mantener la restriccion de monotonia en el clasificador de Bayes:
					 	-siendo L(y,k) = c(y-k) (con c(0)=0)
					 	-la funcion c(k) es convexa si, para todo k entre -(k-1) y (k-1):
					 		c(k) <= (c(k-1)+c(k+1))/2
					 	-el clasificador de Bayes es monotono si y solo si c(k) (que es
					 	 la V-shaped loss function) es convexa.

		-APROXIMACIÓN PLUG-IN:
			-los metodos no parametricos son asi llamados porque explotan la clase de todas las
			 funciones monotonas. Estos son los que consideraremos a partir de ahora en el
			 paper. Estos metodos no hacen ninguna asuncion mas sobre el modelo que la de 
			 las restricciones monotonicas.
			-hemos de construir un metodo para estimar P(y|x), sabiendo que P(y|x) posee
			 dos ventajas:
			 	1-la distribucion condicional permite la determinacion de la prediccion 
			 	  optima para cualquier funcion de perdida.
			 	2-la distribucion condicional mide la confianza de la prediccion
			-el metodo de estimacion esta basado en la regresion isotonica.
			-PROBLEMA DE CLASIFICACION BINARIA Y REGRESION ISOTONICA:
				-en la aproximacion plugin se propone usar un vector de estimadores de densidad
				 condicional p = (p1,....,pn), el cual es una regresion isotonica del vector de
				 etiquetas y = (y1,...,yn). Es decir, p nos da la probabilidad de que x 
				 pertenezca a cada una de las clases existentes en y.
				-p es la solucion del problema: SUM((yi-pi)^2) sujeto a las restricciones de
				 monotonicidad (Xi>=Xj --> pi>=pj). Por ello p minimiza el error cuadratico en 
				 el conjunto de los vectores monotonos p=(p1,..,pn) para cada x.
				-la eleccion de la funcion de error (funcion de perdida de error cuadratico)
				 parece ser arbitraria. Puede verse que haciendo uso de otras funciones
				 de perdida, se llega al mismo resultado.
				-la regresion isotonica es un problema de optimizacion cuadratica con 
				 restricciones lineales, por ello puede ser resuelta de forma eficiente con
				 la mayoria de los resolutores de optimizacion de proposito general.
			-MULTICLASS PROBLEM:
				-basado en la regresion isotonica multiclase
				-la idea es descomponer el problema de K-clases en varios problemas binarios
				 y aplicar regresion isotonica a cada uno de los problemas.
				-Esta demostrado que la descomposicion del problema de estimacion de 
				 probabilidad para el caso de multiclase  siempre forman una adecuada 
				 distribucion de probabilidad, es decir, que siempre son no-negativos
				 y la suma es igual a 1.

		-APROXIMACION DIRECTA
			-consideramos la clasificacion directa basada en la minimizacion del riesgo 
			 empirico dentro de la clase de todas las funciones monotonas.
			-aunque este tipo de funciones no se pueden describir con un numero 
			 finito de parametros, la minimizacion del riesgo puede realizarse debido a 
			 que solo estamos interesados en valores de funciones monotonas en ciertos
			 puntos, los incluidos en D (training set).
			-Una funcion monotona minimizando el riesgo empirico puede obtenerse resolviendo
			 el siguiente problema de optimizacion:
			 	minimizar: SUM(L(yi,di))
			 	sujeto a las restricciones de monotonia.
			 	-donde di son variables del problema (valores de la funcion monotona optima
			 	 en puntos de D)
			-el problema puede tener otra interpretacion interesante: reetiquetar las
			 instancias para hacer el dataset monotono de forma que las etiquetas de las
			 instancias sean lo mas parecidas a las del conjunto original, donde esta 
			 similitud es medida en terminos de la funcion de perdida. Estas nuevas
			 etiquetas seran los nuevos valores optimos de las variables di. 
			 Este reetiquetado puede realizarse en el proceso de preprocesamiento
			 y corresponde a la correccion del error no parametrico.
			-como el problema de clasificacion no parametrica se asimila al de la regresion
			 isotonica (exceptuando que ahora se considera una salida discreta), será llamado
			 ahora "clasificacion isotonica" y su solucion optima d^ será llamada
			 "clasificacion optima de y"

___________________________________________________________________________________________________
	
	-1811.07155: monotonic classification: an overview on algorithms,
				 performance measures an data sets.

		-ejemplos de uso de monotonicidad:
			-comparacion de dos compañias donde una domina sobre la otra en terminos de
			 TODOS los indicadores financieros. Debido a esto, la compañia dominante
			 ha de tener una evaluacion final superior a la compañia dominada. Un uso
			 de esto, es la prediccion de la calificacion crediticia usada por los bancos.
			-House pricing: el precio de una casa sera superior cuantas mas habitaciones
			 posea, mejor sea la calidad del aire acondicionado y menor sea la polución
			 en el ambiente.

		-la consideracion de restricciones monotonicas queda motivada por dos hechos:
			1-el tamaño del espacio de la hipotesis es reducido, lo que facilita el
			  proceso de aprendizaje. (¿existencia de un numero reducido de clases?)
			2-otras metricas ademas de la precision, como la consistencia con respecto
			  a estas restricciones, pueden ser usadas por los expertos para aceptar
			  o rechazar el modelo. Estas técnicas de evaluacion de restricciones
			  monotónicas las veremos mas adelante con el fin de poder evaluar 
			  la consistencia de estas.

		-DEFINICION DE CLASIFICACION MONTONICA:
			-una restriccion monotonica siempre involucra  un atributo de entrada y el
			 atributo de clase, y ha de haber, al menos, una restriccion de monotonicidad
			 para distinguir entre clasificacion ordinal y monotonica.
			-la clasificacion monotonica puede ser directa (más habitaciones, precio mayor
			 de una casa), o inversa (más polucion, precio menor de la casa).
			-Normalmente en problemas de clasificacion monotonica reales, las restricciones
			 monotonicas son consideradas en un subconjunto de caracteristicas del dataset.
			-DESCRIPCION FORMAL DEL PROBLEMA -> muy bien explicada en el paper(apartado 2,
			 pagina 6)

		-TAXONOMIA PARA ALGORITMOS DE CLASIFICACION MONOTONICA
			-categorizacion basada en el proposito de cada metodo, la heuristica que
			 sigue y el modelo generado por cada algoritmo. Division de algoritmos:
			 	1-Clasificadores monotonicos: cuyo objetivo es crear modelos predictivos
			 	  que satisfagan las restricciones monotonicas de forma parcial o total.
			 	  Clasificadores en funcion del modelo que construyen:
			 	  		-clasificadores basados en instancias: no crean un modelo, directamente
			 	  		 utilizan las instancias del data set para la realizacion de decisiones
			 	  		-arboles de decision o reglas de clasificacion: los modelos creados
			 	  		 involucran reglas de produccion legibles en forma de arboles de
			 	  		 decision o conjunto de reglas.
			 	  		-Ensembles o multiclasificadores: uso de varios clasificadores para
			 	  		 obtener diferentes respuestas y posteriormente agregarlas en una
			 	  		 decision de clasificacion global:
			 	  		 	-boosting
			 	  		 	-bagging
			 	  		-redes neuronales: modelo bioinspirado en el que la funcion que
			 	  		 relaciona las entradas con la salida son bloques de construccion
			 	  		 (neuronas) interconectadas entre si y organizadas en capas.
			 	  		 Mediante un proceso de entrenamiento interativo se alcanzan
			 	  		 los pesos adecuados para cada conexion.
			 	  		-support vector machine: aprendizaje basado en maquinas de soporte
			 	  		 vectorial y derivadas
			 	  		-hibrido: combinacion de varios algoritmos (rules + instance based
			 	  		 learning i.e.)
			 	  		-fuzzy integral: basados en el uso de la integral de Choquet (para
			 	  		 medir la utilidad esperada de un evento incierto). Puede ser vista
			 	  		 como una generalizacion de la integral estandar de Lebesque para
			 	  		 el caso de medidas no aditivas.
			 	2-Preprocesamiento monotonico: refina los conjuntos de datos para mejorar el
			 	  rendimiento de los clasificadores monotonicos.
			 	  		-relabeling: para minimizar el numero de violaciones de monotonicidad
			 	  		 presentes en el data set. (casos en los que la variable de salida
			 	  		 no concuerda con las restricciones de monotonicidad)
			 	  		-feature selection: obtener los atributos mas relevantes para mejorar
			 	  		 el rendimiento de la clasificacion monotonica
			 	  		-instance selection: seleccion de subconjunto de instancias del 
			 	  		 data set con el objetivo de derivar en mejores clasificadores
			 	  		 monotonicos
			 	  		-seleccion de conjunto de entrenamiento: la heuristica seguida por
			 	  		 estos algoritmos ha de ser generica de forma que el subconjunto
			 	  		 seleccionado sea el que mejores resultados produce sin importar el 
			 	  		 clasificador utilizado posteriormente.
			 	 
			-Ejemplos de algoritmos basados en arboles:
				-MID(Monotonic induction of decision trees):
					-uso del arbol de clasificacion ID3.
					-Ben-David introduce una medida de monotonicidad denotada como
					 marcador-totalmente-ambiguo
					-para calcularla se construye una matriz bxb no monotonica, concerniente
					 a un arbol que contiene b ramas.
					-cada valor mij es 1 si las ramas i y j son no monotonas y 0 si lo son.

				-Positive/Quasi-Positive Decision Tree:
					-la regla de separacion consiste en separar los puntos que tienen
					 el hijo derecho mayor que el izquierdo (en terminos de la variable
					 de salida)
					-el algoritmo añade muestras a los nodos de forma que el arbol resultante
					 es monotono.
					-se requiere aplicacion sobre data sets estrictamente monotonos y binarios
					 (que contengan solo dos clases)

				-Monotonic Decision Tree:
					-pensado para generar arboles de decision motononicos a partir de conjuntos
					 de muestras que pueden no se monotonicos o consistentes.
					-construye el arbol usando un conjunto de etiquetas ordinales que no son
					 las mismas que las originales.
					-puede usarse un proceso de mapeo para reetiquetarlas a las originales

				-Isotonic classification tree:
					-ajusta la probabilidad estimada en los nodos hoja en caso de una violacion
					 de la monotonicidad
					-idea: considerando la restriccion de monotonicidad, la suma de la 
					 prediccion absoluta de errores en la muestra de entrenamiento ha de ser
					 minimizada
					-ademas, el algoritmo puede soportar problemas donde algunos, pero no todos
					 los atributos, tienen una relacion monotonica con respecto a la variable
					 dependiente

			-preprocesamiento monotonico:
				-Relabeling: cambio de etiqueta para las instancias que producen violacion
							 de las restricciones monotonicas que producen conjuntos de
							 datos completamente monotonos, los cuales son necesarios
							 para muchos clasificadores monotonicos.
						-Dykstra Relabel: relabeling basado en la regresión monotonica, 
										  capaz de minimiazr el error absoluto o el 
										  error cuadratico.
										  Es optimo con estas loss functions pero no
										  garantiza el numero minimo de cambio de 
										  etiquetas debido a que no es el objetivo.
						-Daniels-Velikova Greedy Relabel: algoritmo greedy para reetiquetar
										  ejemplos no monotonos uno por uno. En cada iteracion
										  busca una instancia y la nueva etiqueta para
										  maximizar la monotonicidad del data set.
										  Aunque en cada iteracion es capaz de maximizar el
										  salto hacia la completa monotonicidad, el algoritmo
										  reetiqueta mas ejemplos de los necesarios.
										  No garantiza solucion optima.
						-Optimal Flow Network Relabel: basado en encontrar un conjunto 
										  independiente de pesos maximo en el grafo de 
										  violacion de monotonicidad. Reetiquetando el
										  complemento de este conjunto, resulta en un 
										  data set monotono con los menores cambios posibles.
										  Este metodo es optimo, produciendo el menor numero
										  de cambios posibles. (Optimo?)
						-Single-pass Optimal Ordinal Relabel: la idea es explorat las 
										  propiedades de una red de flujo minima e identificar
										  propiedades agradables de algunos cortes maximos.
										  Es un algoritmo optimo de reetiquetado.
				-Feature Selection: mejorar capacidad predictiva de clasificadores monotonicos
									seleccionando las caracteristicas mas relevantes.
						-O-ReliefF/O-Simba: Los autores introducen algoritmos de selección de
							 características basados en márgenes para la clasificación monotónica al incorporar las restricciones de monotonicidad en la
							 tarea ordinal. Los métodos ReliefF y Simba se extienden al
							 contexto de la clasificación ordinal.
						-min-Redundancy max-Relevance(mRMR): integra el ranking de la metrica
							de informacion mutua con la estrategia de busqueda de la minima
							redundancia y la maxima relevancia, creando un algoritmo efectivo
							para el objetivo propuesto.
						-Non-monotonic feature selection via Multiple Kernel Learning:	
							seleccion de caracteristicas no monotonica con alivio de violacion
							de restricciones monotonicas mediante la computacion de socores 
							para cada caracteristica individual que depende de la cantidad
							de caracteristicas selccionadas.
				-Instance Selecion: mediante el uso de heuristicas basadas en instancias, se
									pretende mejorar el rendimiento de los clasificadores
									monotonicos escogiendo aquellas instancias más relevantes
									para el caso de estudio.
						-Monotonic Iterative Prototype Selection (MONIPS):
							Sigue un esquema iterativo en el que determina las instancias más
							representativas que mantienen o mejoran las capacidades predictivas
							del algoritmo MkNN. Realiza un borrado de instancias basado en la
							mejora del rendimiento de MkNN.
				-Training Set Selection: estos algoritmos poseen el mismo objetivo que los 
										anteriores, con la única diferencia de que la 
										heuristica seguida ha de ser generica de tal forma que 
										el subconjunto seleccionado ha de ser el que 
										proporcione el mejor rendimiento independientemente del
										clasificador que se utilice más tarde sobre él.
						-Monotonic Training Set Selection(MonTSS):
							Incorpora las medidas necesarias para identificar y seleccionar
							las instancias apropiadas en el conjunto de datos para mejorar
							tanto el accuracy como la naturaleza monotonica de los modelos
							producidos por diferentes clasificadores.

		-QUALITY METRICS USED IN MONOTONIC CLASSIFICATION (PAGINA 23)
			-metricas de evaluacion predictiva:
				-Clasificacion binaria:
					-accuracy: representa la habilidad predictiva de acuerdo con la proporción de 
							   los datos clasificados de forma correcta.
					-error rate: caso opuesto al accuracy, evaluando la proporcion de los datos
								 evaluados clasificados de forma incorrecta.
					-recall/sensitivity: medida de la proporcion de TP que estan correctamente clasificados
					-false positive rate: FP/FP+TN
					-positive predictive value(PPV) / precision: proporcion de las instancias del test que tienen una
													 salida positiva y que ademas están bien clasificados.
													 representa la probabilidad de que una prueba positiva 
													 refleje la condición subyacente que se está probando.
					-negative predictive value(NPV): proporcion de instancias de test con valores de prediccion
													 negativos que estan clasificados de forma correcta.
					-F-Measure: es la media armónica (inverso de media aritmética) de las medidas precision y recall.
					-Arean Under Curve(AUC): para combiniar Recall y false positive rate en una sola medida, primeramente
											 calculamos cada una de estas medidas con varios thresold distintos para la
											 regresión logística y después los ploteamos en una sola gráfica con 
											 el ratio de falsos positivos en las abcisas y el Recall en el eje de
											 ordenadas. El resultado es la curva ROC, y la métrica que consideramos es 
											 el área bajo esta curva.
				-Clasificación multiclase:
					-Mean Squared Error: mide la media de los cuadrados de los errores
					-Mean Absolute Error: mide como de cerca se encuentran las predicciones de los valores reales de salida (igual que MSE pero sin el cuadrado)
					-Monotonic Accuracy: igual que accuracy pero tan solo usando las instancias que cumplen los requisitos de monotonia en el conjunto de test.
					-Monotonic Mean Absolute Error: igual que MAE, pero considerando solo las instancias que cumplen los requisitos de monotonia en el conjunto de test.

			-metricas de cumplimiento de monotonicidad: el interés consiste en evaluar el ratio de monotonicidad provisto por las 
														predicciones obtenidas o por el modelo construido.
														NClash(x) = numero de instancias del data set que NO cumplen las restricciones de monotonia con respecto a la instancia x
														NMonot(x) = numero de instancias del data set que SI cumplen las restricciones de monotonia con respecto a la instancia x

							-el indice de no monotonicidad (NMI) es el numero de parejas de choque (parejas que no cumplen las restricciones monotonicas) dividido por el numero
							 total de parejas en el data set.
							-frecuencia de monotonicidad (FOM) es la suma de las parejas que cumplen las restricciones de moonotonicidad dividido entre el número total de parejas.

		MEDIDAS CON PROPOSITOS PREDICTIVOS MAS USADAS:
			-MAE
			-NMI

_______________________________________________________________________________________________________________________________________________________________________________________
_______________________________________________________________________________________________________________________________________________________________________________________
_______________________________________________________________________________________________________________________________________________________________________________________

Literatura generica de data streams:
	-Gama_2012_article.....:A survey on learning from data streams: current and future trends
		-Los sistemas tradicionales basados en el uso de memoria, entrenados de una forma fija mediante conjuntos de entrenamiento y los cuales generan modelos estáticos no están
		 preparados para procesar los datos altamente detallados disponibles en procesos como, por ejemplo, el continuo análisis del consumo de energía de una red electrica en distintas zonas
		 en el espacio, lo cual genera una gran cantidad de datos que ha de ser procesada de forma rápida con el fin de generar modelos predictivos consistentes
		 que se adapten a situaciones cambiantes y puedan reaccionar de forma rápida y eficaz a dichos cambios.

		 -MACHINE LEARNING DATA STREAMS:
		 	-el Machine Learning extrae conocimiento en forma de modelos y patrones de unos datos de naturaleza cambiante. Hoy en día la generación de datos, gracias a las capacidades
		 	 tecnológicas de las que disfrutamos, se produce a altas velocidades, tanto así, que se pone, en cuanto a velocidad, por delante del procesamiento de dichos datos, lo cual
		 	 quiere decir que generamos datos a mayor velocidad de lo que las capacidades computacionales que tenemos ahora mismo nos permiten procesarlos.
		 	 Desde este punto de vista, conviene modelar los datos como flujos de datos transitorios en lugar de como tablas de datos persistentes.

		 	 HACER CAPTURA DE PANTALLA DE LA TABLA 1 EN LA PAGINA 2 del PDF

		 	 -Streaming algorithms:
		 	 	-2 modelos distintos:
		 	 		-insert-only model: entran datos al sistema de forma secuencial.
		 	 		-insert-delete model: los elementos que entran pueden ser eliminados o actualizados.
		 	 	-desde el punto de vista de los sistemas de control de flujo de datos(DSMS), existen varios problemas que requieren técnicas de procesamiento aproximadas para evaluar el flujo 
		 	 	 continuo de datos que requieren una cantidad ilimitada de memoria.
		 	 	-los algoritmos que procesan flujos de datos producen soluciones aproximadas dentro de un rango de error con una alta probabilidad, relajando así las restricciones a la hora
		 	 	 de obtener una solución exacta.
		 	 	-en algunas aplicaciones una respuesta aproximada deberia estar dentro de un rango de error admisible. Los sistemas de control de flujos de datos han desarrollado un conjunto
		 	 	 de técnicas que almacenan resumenes de datos compactos suficientes para resolver consultas. Estas aproximaciones requieren un equilibrio entre el accuracy y la cantidad de
		 	 	 memoria usada para almacenar los resumenes, con una restricción adicional de tiempo de procesado de los datos.
		 	 -Aproximacion y aleatorización
		 	 	Dentro del marco del data streaming, como ya hemos dicho, está permitido ofrecer respuestas aproximadas dentro de un pequeño rango de error (e), con una pequeña probabilidad
		 	 	de fallo (d) para obtener respuestas con una probabilidad de que 1-d se encuentre en el intervalo de radio e.
		 	 	Los algoritmos que usan dichas aproximacióon y aleatorización son referidos por dichos (e,d).
		 	 	la idea basicamente consiste en mapear cada espacio grande de entrada en una sinopsis pequeña.
		 	 	La aproximación y randomización han sido usadas en solventar problemas como mineria de reglas de asociación, items frecuentes, k-means...
		 	 -Ventanas de tiempo	
		 	 	-Para la realización del cómputo estadístico referente al modelo de flujos, no nos interesa el total de los datos existentes, si no los más recientes, entendiendo que son los
		 	 	 que mejor explican la situación a la que nos enfrentamos y pudiendo, de esta forma, deshacernos de grandes cantidades de datos que no nos son útiles.
		 	 	 Las técnicas más simples en cuanto a este tipo de tratamiento de datos, utilizan una ventana deslizante de tamaño fijo, con un funcionmiento FIFO (first in fist out).
		 	 	 Definimos dos tipos de ventana deslizante:
		 	 	 	-basada en sequencia: tamaño de ventana definido por el número de observaciones (tamaño fijo o variable en el tiempo)
		 	 	 	-basada en marca de tiempo: tamaño de ventana definido en términos de duración. Una ventana de este tipo de tamaño t consiste en todos los elementos cuya marca de tiempo
		 	 	 								se situa dentro del intervalo de tiempo t del actual periodo de tiempo.
		 	 	 El hecho de monitorizar, analizar y extraer conocimiento de flujos de datos de alta velocidad, puede hacer que existan diversos niveles de granularidad a la hora de 
		 	 	 almacenar los datos. Conforme más antiguos son los datos que disponemos, mayor granularidad requeriremos en la información (es decir, menor precisión). Cuanto más
		 	 	 reciente sean los datos, el grano ha de ser más fino, ya que requerimos más precisión al tratarlos debido a que son más importantes (este es llamado el modelo de ventana de 
		 	 	 tiempo inclinado).	
		 	 	 AdWin-ADaptive sliding WINdow: mantiene una ventana variable con respecto a los items recientemente vistos con la propiedad de que la ventana tiene un tamaño maximal
		 	 	 estadísticamente consistente con la hipótesis de que no haya habido un cambio en la media del valor dentro de la ventana. Un fragmento viejo de la ventana se desecha
		 	 	 si hay alguna evidencia de que tiene un valor distinto al del resto de la ventana.
		 	 -sampling	
		 	 	-seleccion del subconjunto de datos a analizar en intervalos periodicos.
		 	 	-se usa para calcular estadisticas del flujo (valores esperados)
		 	 	-reducen coste cantidad de datos a procesar, por tanto, el computacional
		 	 	-pueden ser una fuente de errores, por ejemplo, en aplicaciones dedicadas a la detección de valores extremos o anomalias
		 	 	-el problema principal es obtener una muestra representativa
		 	 	-tecnicas:
		 	 		-random sampling
		 	 		-reservoir sampling
		 	 		-load shedding
		 	 -sinopsis bocetos y resumenes
		 	 	-sinopsis: estructuras de datos compactas que resumen datos para su posterior consulta.
		 	 	-data sketching: herramienta de reducción de dimensionalidad. Usa proyecciones aleatorias de datos con cierta dimensión d a un espacio de cierto conjunto de dimensiones.
		 	 	-data stream summary (by Cormode and Muthukrishnan): usado para aproximaciones (e,d) para resolver consultas de rango, consultas puntuales y consultas innerproduct.

		 -ALGORITHMS FOR LEARNING FROM DATA STREAMS
		 	-REFERENCIA:
		 	 Hulten,G.,Domingos,P.:Catchingupwiththedata:researchissues in mining data streams. In: Proceedings of Workshop on Research Issues in Data Mining and Knowledge Discovery, Santa Baraba, USA (2001) 
		 	-Hulten y Domingos presentan un metodo general para aprender de bases de datos grandes y arbitrarias:
		 		-consiste en derivar una límite superior para la pérdida del learner en función del número de ejemplos usados en cada paso del algoritmo.
		 		 De esta forma, se minimiza el número de ejemplos requeridos en cada paso del algoritmo, a la vez que se garantiza que el modelo obtenido
		 		 no difiere de forma significante de aquel que se obtendría con todos los datos.
		 		-esta metodologia de datos se ha aplicado de forma exitosa en k-means, clustering jerárquico de variables, árboles de decision, etc.
		 	-El aprendizaje de conjuntos de datos grandes puede ser más efectivo con el uso de algoritmos que ponen más énfasis en la gestión del sesgo (bias).
		 	-Uno de estos algoritmos es el Very Fast Decission Tree system (VFDT): es un algoritmo de aprendizaje basado en árboles de decisión que ajusta de forma dinámica su sesgo en función
			 	 de que haya nuevos ejemplos disponibles.
			 	 En la inducción del árbol de decisión, el principal problema consiste en tomar la decisión de cuando expandir el árbol, instalando un test de división y generando nuevas hojas.
			 	 La idea básica consiste en usar un conjunto pequeño de ejemplos para seleccionar el test de deivisón para colocar en un nodo del arbol de decisión.
			 	 Si tras ver un conjunto de ejemplos, la diferencia en resultados entre ambos test de división no satisface un test estadistico (Hoeffding bound), entonces VFDT procede a examinar
			 	 más ejemplos.
			 	 Esto solo toma una decisión, cuando hay evidencias estadísticas suficientes a favor de un test particular.
			 	 Esta estrategia satisface la estabilidad del modelo (baja varianza), controla el overfitting, mientras que puede alcanzar un numero acrecentado de grados de libertad (low bias)
			 	 incrementando el número de ejemplos.
		 	 	 En VDFT se aprende un árbol de decisión de forma recursiva reemplazando hojas por nodos de decisión. Cada hoja almacena las estadísticas necesarias sobre los valores de los
		 	 	 atributos. Dichas estadísticas necesarias son aquellas que se necesitan por una función de evaluación heurística que realiza el cálculo de el resultado de los test de división 
		 	 	 basada en el valor de los atributos.
		 	 	 Cuando hay un ejemplo disponible, atraviesa el árbol desde la raíz hasta una hoja evaluando el atributo requerido en cada nodo y siguiendo la rama correspondiente al valor
		 	 	 del atributo en el ejemplo.
		 	 	 Cuando el ejemplo llega a una hoja, la estadística de las hojas por las que ha pasado han sido actualizadas.
		 	 	 Entonces cada condición basada en los valores de los atributos ha sido evaluada.
		 	 	 El nuevo nodo de decisión tendrá tantos descendientes como el número de posibles valores tenga el atributo escogido (por lo que el árbol no es necesariamente binario).
		 	 	 Los nodos de decisión tan solo contienen la información sobre el test de división instalado en ellos.
		 	 	 La innovación principal de los VFDT es el uso de los Hoeffding bounds para decidir cuantos ejemplos han de verse previa instalación de un test de decisión en un nodo
		 	 	 hoja.
		 	-VFDT ha sido implementado también para tratar con atributos continuos, hojas funcionales y con flujos de datos no estacionarios. 
		 	-el CVFDT es una extensión para flujos de datos cambiantes en el tiempo. Este genera árboles de decisión alternativos en nodos donde hay evidencia de que el test de división no
		 	 es ya apropiado. El sistema reemplaza el árbol actual con uno nuevo que resulta ser más preciso.
		 	-Una característica interesante de VFDT es que tiene la capacidad de congelar las hojas menos comprometedoras en entornos de trabajo con memoria limitada.
		 	-existen ejemplos de VFDT con bagging y boosting y para aprender regresión y modelos de árboles.


		 -PROBLEMAS DE LOS ALGORITMOS AL APRENDER DE FLUJOS DE DATOS
		 	-el objetivo de la minería de datos es la habilidad de mantener de forma permanente un modelo de decisión preciso.
		 	 Este problema requiere algoritmos que se adapten a los datos conforme esten disponibles para poder aprender de ellos.
		 	 Además, los datos desactualizados han de ser olvidados para dejar de tenerlos en cuenta a la hora de crear el modelo, cosa que ha de ocurir en la presencia de 
		 	 información con una distribución no estacionaria presente. El aprendizaje en flujos de datos requiere por tanto algoritmos incrementales de aprendizaje que tengan en
		 	 cuenta el llamado "concept drift".
		 	 La solución a estos problemas requieren nuevas técnicas de muestreo y randomización, y nuevos algoritmos aproximados, incrementales y decrementales.
		 	 Algunas propiedades deseables para algoritmos de flujos de datos:
		 	 	-incrementalidad
		 	 	-aprendizaje online
		 	 	-tiempo constante de procesado de cada ejemplo
		 	 	-un solo escaneo sobre el conjunto de datos de training
		 	 	-tener en cuenta el concept drift

		 	-Los algoritmos de aprendizaje incrementales y decrementales requieren una permanente actualización del modelo de decisión conforme llegan datos nuevos. Esta habilidad 
		 	 de actualizar el modelo mediante las propiedades de los nuevos datos es importante, pero no suficiente, ya que también es necesaria la habilidad de olvidar información
		 	 anticuada para dar un giro en el aprendizaje realizado, dejando de tener en cuenta los items antiguos: decremental learning.
		 	 Evidentemente existe una balanza entre la ganancia en rendimiento ofrecido por el algoritmo y la manutención de la característica de actualización de este, lo que hace que 
		 	 el cómputo realizado por el algoritmo sea más complejo. Ante esta balanza, con el fin de no acrecentar el cómputo, haciendo que el algoritmo decida de forma dinámica
		 	 qué información borrar y cuál no, surge la técnica de ventana deslizante.

		 	-De forma general, es complicado asumir que, en el manejo de flujos de datos durante un largo tiempo, estemos tratando con datos acordes a una distribución de probabilidad
		 	 estacionaria. En sistemas complejos y en largos periodos de tiempo, debemos esperar cambios en la distribución de los items.
		 	 Una aproximación natural para estas tareas incrementales son los algoritmos de aprendizaje adaptativo, algoritmos incrementales que tienen en cuenta el concept drift.
		 	 El concept drift en sí, se refiere al cambio de concepto que sufren los datos a la largo del tiempo, cada vez con cierta permanencia mínima.
		 	 Hay algoritmos que implementan el olvido de información antigua teniendo en cuenta este cambio de concepto, lo que los hace mucho más precisos que los propios algoritmos
		 	 que realizan la eliminación de información en forma de tamaño de ventana prefijado.
		 	 Con el uso de los algoritmos de detección de concept drift podemos averiguar cuando y por qué ha cambiado el comportamiento del flujo de datos.
		 	 Estos algoritmos no poseen la información de mundo cerrado de la que disponen los algoritmos convencionales para el tratamiento de datos estáticos, si no que han de ser
		 	 capaces de adaptarse a un munto abierto cambiante de datos para diferenciar entre cambio de concepto y ruido en los datos.

		 -A la hora de evaluar los resultados en el contexto de los flujos de datos, es interesante tener en cuenta la evolución del acierto de nuestro algoritmo a lo largo del tiempo
		  con los cambios de concepto acaecidos.


___________________________________________________________________________________________________


	-bifet10a---->MOA: Massive Online Analysis
		-MOA es un entorno de trabajo open-source relacionado con el proyecto WEKA (Waikato Environment for Knowledge Analysis) para el tratamiento o minería de flujos de datos de evolución masiva conteniendo una gran colección de algoritmos de machine
		 learning como son: classificación, regresión, clustering, detección de outliers, detección de concept drift y sistemas de recomendación, así como herramientas de
		 evaluación. 
		-Requisitos para un entorno de análisis de flujos de datos:
			1-Procesamiento de un ejemplo en cada instante de tiempo e inspección de este tan solo una vez.
			2-Limitado uso de memoria.
			3-Trabajo en un tiempo limitado.
			4-Estar listo para predecir en cualquier momento.
		-Ciclo de clasificación de flujo de datos: (COGER LA IMAGEN)
			1-El algoritmo toma el siguiente ejemplo del flujo
			2-El algoritmo procesa el ejemplo actualizando sus estructuras de datos. En este punto no se ha de exceder los límites de memoria y ha de ser lo más rápido posible.
			3-El algoritmo está listo para aceptar el siguiente ejemplo.
		-Para el procedimiento de evaluación de los algoritmos de aprendizaje, mientras que los modelos batch tradicionales utilizan un conjunto de datos de train y otro de 
		 test de reserva (holdout) para realizar dicha evaluación, en los algoritmos dedicados al data streaming se utiliza el método  Interleaved Test-Then-Train o Prequential,
		 mediante el cual cada una de las instancias que se reciben se utilizan como instancia de test para, posteriormente, usarla como nuevo dato de aprendizaje (train).
		 De esta forma no es necesario mantener un conjunto de datos de reserva exclusivo para validar, haciendo que el uso de los datos disponibles sea máximo, además
		 de que ayuda a crear una representación más visual de la evolución de la precisión del algoritmo a lo largo del tiempo.
		


 	 