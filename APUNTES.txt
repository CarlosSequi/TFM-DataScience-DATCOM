Literatura generica de monotonicos:
	-Kotlowski:
		-funcion monotona: una funcion es monotona si y solo si x<=y implica f(x)<=f(y)
		-clasificacion ordinal con restricciones monotonicas: maneja conocimiento subyacente del
		 problema sobre clases ordenadas, atributos ordenados y una relacion monotonica entre
		 la evaluacion de los atributos de una instancia y la asignacion de esta a una clase.
		-principio de dominancia: a mayor valor en atributos de una instancia, mayor sera el
		 valor de la clase a la que se asigna dicha instancia (clasificacion ordinal con
		 restricciones monotonicas)
		-si no hay relacion de monotonia en la asociacion de una clase a una instancia, 
		 pero las clases si poseen un orden se considera clasificacion ordinal tan solo
		-con las restricciones de monotonia presentes se puede trabajar con una amplia
		 variedad de funciones sin temor a que introduzcan más restricciones que 
		 la de monotonia: es posible hacer inferencia de la clase sobre todas las funciones
		 monotonas.
		-proposito del paper:
			-formalizar la aproximacion de clasificacion con restricciones monotonicas
			-analizar los metodos no parametricos de clasificacion
				-plug-in: estimando la distribucion condicional de la clase. Provienen de la
				 clasificacion isotonica (monotona creciente o decreciente)
				-empirical risk (direct approach): minimizando el riesgo empirico
		-usando el termino de "relacion de dominancia" decimos que una instancia x domina a
		 otra instancia x' cuando cada una de las variables de entrada de x (atributos de x)
		 son mayores o iguales que cada uno de los de x', se denota x>=x' y por tanto
		 x tendra asignada una etiqueta de clase mayor que x'.
		-una funcion es monotona si x>=x' -> h(x)>=h(x'). Es decir, si x domina a x', la
		 inferencia de clase de x ha de ser superior a la de x'.
		-RESTRICCIONES MONOTONICAS:
			-seran restricciones con respecto a la probabilidad de distribución de los 
			 datos y con repecto a las imposiciones de la función de perdida bajo las cuales
			 el clasificador optimo de Bayes es monotono.
			-DOMINANCIA ESTOCASTICA:
				-la restriccion de monotonia comentada (si  x>=x' -> h(x)>=h(x')) no siempre
				 se aplica en la practica de forma tan restrictiva, por ello hemos de 
				 hablar en términos probabilisticos a la hora de referirnos a dichas
				 restricciones
				-Decimos entonces que, siendo 'k' una de las posibles clases a tomar en el 
				 dominio por una instancia 'x', y siendo 'y' la etiqueta asignada a una
				 instancia 'x', si la restricción monótona nos dice que x>=x', entonces
				 la dominancia estocastica nos dice que P(y<=k|x)<=P(y<=k|x').
				 Es decir, que la probabilidad de que el valor asignado (y) a la instancia 
				 dominante (x) sea mayor que cierto valor de la clase fijado (k), es mayor
				 que la probabilidad de que el valor asignado (y) a la instancia dominada(x')
				 sea mayor que ese mismo cierto valor de la clase fijado.
				-la relacion de dominancia estocastica entre distribuciones se denota asi:
					x >= x' -> P(y|x) >= P(y|x')
			-CLASIFICADOR MONOTONO DE BAYES
				-en el problema de clasificacion el objetivo es encontrar el clasificador más
				 parecido al clasificador de Bayes, es decir, esta es nuestra función objetivo.
				-Siendo esta nuestra funcion objetivo, es un requisito que tambien aplique
				 las restricciones de monotonía que hemos enunciado. Problema: aunque la
				 distribucion de probabilidad tiene restricciones monotonicas, el clasificador
				 de Bayes no siempre las mantiene.
				-para solucionar este problema y mantener la monotonia en el clasificador de 
				 Bayes, han de imponerse las siguientes restricciones a la funcion de perdida
				 (L):
				 	-L(y,k+1)-L(y,k) >= L(y+1,k+1)-L(y+1,k)
					 	-demostrado en  Computer Society Digital Library at 
					 	http://doi.ieeecomputersociety.org/10.1109/TKDE.2012.204
					 	-esta caracteristica de la funcion de perdida es necesaria
					 	 en la clasificacion con restricciones monotonicas, si no no
					 	  tendria sentido minimizar el riesgo dentro de la clase de las funciones monotonas.
					-la siguiente definicion de convexidad es necesaria tambien para
					 mantener la restriccion de monotonia en el clasificador de Bayes:
					 	-siendo L(y,k) = c(y-k) (con c(0)=0)
					 	-la funcion c(k) es convexa si, para todo k entre -(k-1) y (k-1):
					 		c(k) <= (c(k-1)+c(k+1))/2
					 	-el clasificador de Bayes es monotono si y solo si c(k) (que es
					 	 la V-shaped loss function) es convexa.

		-APROXIMACIÓN PLUG-IN:
			-los metodos no parametricos son asi llamados porque explotan la clase de todas las
			 funciones monotonas. Estos son los que consideraremos a partir de ahora en el
			 paper. Estos metodos no hacen ninguna asuncion mas sobre el modelo que la de 
			 las restricciones monotonicas.
			-hemos de construir un metodo para estimar P(y|x), sabiendo que P(y|x) posee
			 dos ventajas:
			 	1-la distribucion condicional permite la determinacion de la prediccion 
			 	  optima para cualquier funcion de perdida.
			 	2-la distribucion condicional mide la confianza de la prediccion
			-el metodo de estimacion esta basado en la regresion isotonica.
			-PROBLEMA DE CLASIFICACION BINARIA Y REGRESION ISOTONICA:
				-en la aproximacion plugin se propone usar un vector de estimadores de densidad
				 condicional p = (p1,....,pn), el cual es una regresion isotonica del vector de
				 etiquetas y = (y1,...,yn). Es decir, p nos da la probabilidad de que x 
				 pertenezca a cada una de las clases existentes en y.
				-p es la solucion del problema: SUM((yi-pi)^2) sujeto a las restricciones de
				 monotonicidad (Xi>=Xj --> pi>=pj). Por ello p minimiza el error cuadratico en 
				 el conjunto de los vectores monotonos p=(p1,..,pn) para cada x.
				-la eleccion de la funcion de error (funcion de perdida de error cuadratico)
				 parece ser arbitraria. Puede verse que haciendo uso de otras funciones
				 de perdida, se llega al mismo resultado.
				-la regresion isotonica es un problema de optimizacion cuadratica con 
				 restricciones lineales, por ello puede ser resuelta de forma eficiente con
				 la mayoria de los resolutores de optimizacion de proposito general.
			-MULTICLASS PROBLEM:
				-basado en la regresion isotonica multiclase
				-la idea es descomponer el problema de K-clases en varios problemas binarios
				 y aplicar regresion isotonica a cada uno de los problemas.
				-Esta demostrado que la descomposicion del problema de estimacion de 
				 probabilidad para el caso de multiclase  siempre forman una adecuada 
				 distribucion de probabilidad, es decir, que siempre son no-negativos
				 y la suma es igual a 1.

		-APROXIMACION DIRECTA
			-consideramos la clasificacion directa basada en la minimizacion del riesgo 
			 empirico dentro de la clase de todas las funciones monotonas.
			-aunque este tipo de funciones no se pueden describir con un numero 
			 finito de parametros, la minimizacion del riesgo puede realizarse debido a 
			 que solo estamos interesados en valores de funciones monotonas en ciertos
			 puntos, los incluidos en D (training set).
			-Una funcion monotona minimizando el riesgo empirico puede obtenerse resolviendo
			 el siguiente problema de optimizacion:
			 	minimizar: SUM(L(yi,di))
			 	sujeto a las restricciones de monotonia.
			 	-donde di son variables del problema (valores de la funcion monotona optima
			 	 en puntos de D)
			-el problema puede tener otra interpretacion interesante: reetiquetar las
			 instancias para hacer el dataset monotono de forma que las etiquetas de las
			 instancias sean lo mas parecidas a las del conjunto original, donde esta 
			 similitud es medida en terminos de la funcion de perdida. Estas nuevas
			 etiquetas seran los nuevos valores optimos de las variables di. 
			 Este reetiquetado puede realizarse en el proceso de preprocesamiento
			 y corresponde a la correccion del error no parametrico.
			-como el problema de clasificacion no parametrica se asimila al de la regresion
			 isotonica (exceptuando que ahora se considera una salida discreta), será llamado
			 ahora "clasificacion isotonica" y su solucion optima d^ será llamada
			 "clasificacion optima de y"
