Literatura generica de monotonicos:
	-Kotlowski:
		-funcion monotona: una funcion es monotona si y solo si x<=y implica f(x)<=f(y)
		-clasificacion ordinal con restricciones monotonicas: maneja conocimiento subyacente del
		 problema sobre clases ordenadas, atributos ordenados y una relacion monotonica entre
		 la evaluacion de los atributos de una instancia y la asignacion de esta a una clase.
		-principio de dominancia: a mayor valor en atributos de una instancia, mayor sera el
		 valor de la clase a la que se asigna dicha instancia (clasificacion ordinal con
		 restricciones monotonicas)
		-si no hay relacion de monotonia en la asociacion de una clase a una instancia, 
		 pero las clases si poseen un orden se considera clasificacion ordinal tan solo
		-con las restricciones de monotonia presentes se puede trabajar con una amplia
		 variedad de funciones sin temor a que introduzcan más restricciones que 
		 la de monotonia: es posible hacer inferencia de la clase sobre todas las funciones
		 monotonas.
		-proposito del paper:
			-formalizar la aproximacion de clasificacion con restricciones monotonicas
			-analizar los metodos no parametricos de clasificacion
				-plug-in: estimando la distribucion condicional de la clase. Provienen de la
				 clasificacion isotonica (monotona creciente o decreciente)
				-empirical risk (direct approach): minimizando el riesgo empirico
		-usando el termino de "relacion de dominancia" decimos que una instancia x domina a
		 otra instancia x' cuando cada una de las variables de entrada de x (atributos de x)
		 son mayores o iguales que cada uno de los de x', se denota x>=x' y por tanto
		 x tendra asignada una etiqueta de clase mayor que x'.
		-una funcion es monotona si x>=x' -> h(x)>=h(x'). Es decir, si x domina a x', la
		 inferencia de clase de x ha de ser superior a la de x'.
		-RESTRICCIONES MONOTONICAS:
			-seran restricciones con respecto a la probabilidad de distribución de los 
			 datos y con repecto a las imposiciones de la función de perdida bajo las cuales
			 el clasificador optimo de Bayes es monotono.
			-DOMINANCIA ESTOCASTICA:
				-la restriccion de monotonia comentada (si  x>=x' -> h(x)>=h(x')) no siempre
				 se aplica en la practica de forma tan restrictiva, por ello hemos de 
				 hablar en términos probabilisticos a la hora de referirnos a dichas
				 restricciones
				-Decimos entonces que, siendo 'k' una de las posibles clases a tomar en el 
				 dominio por una instancia 'x', y siendo 'y' la etiqueta asignada a una
				 instancia 'x', si la restricción monótona nos dice que x>=x', entonces
				 la dominancia estocastica nos dice que P(y<=k|x)<=P(y<=k|x').
				 Es decir, que la probabilidad de que el valor asignado (y) a la instancia 
				 dominante (x) sea mayor que cierto valor de la clase fijado (k), es mayor
				 que la probabilidad de que el valor asignado (y) a la instancia dominada(x')
				 sea mayor que ese mismo cierto valor de la clase fijado.
				-la relacion de dominancia estocastica entre distribuciones se denota asi:
					x >= x' -> P(y|x) >= P(y|x')
			-CLASIFICADOR MONOTONO DE BAYES
				-en el problema de clasificacion el objetivo es encontrar el clasificador más
				 parecido al clasificador de Bayes, es decir, esta es nuestra función objetivo.
				-Siendo esta nuestra funcion objetivo, es un requisito que tambien aplique
				 las restricciones de monotonía que hemos enunciado. Problema: aunque la
				 distribucion de probabilidad tiene restricciones monotonicas, el clasificador
				 de Bayes no siempre las mantiene.
				-para solucionar este problema y mantener la monotonia en el clasificador de 
				 Bayes, han de imponerse las siguientes restricciones a la funcion de perdida
				 (L):
				 	-L(y,k+1)-L(y,k) >= L(y+1,k+1)-L(y+1,k)
					 	-demostrado en  Computer Society Digital Library at 
					 	http://doi.ieeecomputersociety.org/10.1109/TKDE.2012.204
					 	-esta caracteristica de la funcion de perdida es necesaria
					 	 en la clasificacion con restricciones monotonicas, si no no
					 	  tendria sentido minimizar el riesgo dentro de la clase de las funciones monotonas.
					-la siguiente definicion de convexidad es necesaria tambien para
					 mantener la restriccion de monotonia en el clasificador de Bayes:
					 	-siendo L(y,k) = c(y-k) (con c(0)=0)
					 	-la funcion c(k) es convexa si, para todo k entre -(k-1) y (k-1):
					 		c(k) <= (c(k-1)+c(k+1))/2
					 	-el clasificador de Bayes es monotono si y solo si c(k) (que es
					 	 la V-shaped loss function) es convexa.

		-APROXIMACIÓN PLUG-IN:
			-los metodos no parametricos son asi llamados porque explotan la clase de todas las
			 funciones monotonas. Estos son los que consideraremos a partir de ahora en el
			 paper. Estos metodos no hacen ninguna asuncion mas sobre el modelo que la de 
			 las restricciones monotonicas.
			-hemos de construir un metodo para estimar P(y|x), sabiendo que P(y|x) posee
			 dos ventajas:
			 	1-la distribucion condicional permite la determinacion de la prediccion 
			 	  optima para cualquier funcion de perdida.
			 	2-la distribucion condicional mide la confianza de la prediccion
			-el metodo de estimacion esta basado en la regresion isotonica.
			-PROBLEMA DE CLASIFICACION BINARIA Y REGRESION ISOTONICA:
				-en la aproximacion plugin se propone usar un vector de estimadores de densidad
				 condicional p = (p1,....,pn), el cual es una regresion isotonica del vector de
				 etiquetas y = (y1,...,yn). Es decir, p nos da la probabilidad de que x 
				 pertenezca a cada una de las clases existentes en y.
				-p es la solucion del problema: SUM((yi-pi)^2) sujeto a las restricciones de
				 monotonicidad (Xi>=Xj --> pi>=pj). Por ello p minimiza el error cuadratico en 
				 el conjunto de los vectores monotonos p=(p1,..,pn) para cada x.
				-la eleccion de la funcion de error (funcion de perdida de error cuadratico)
				 parece ser arbitraria. Puede verse que haciendo uso de otras funciones
				 de perdida, se llega al mismo resultado.
				-la regresion isotonica es un problema de optimizacion cuadratica con 
				 restricciones lineales, por ello puede ser resuelta de forma eficiente con
				 la mayoria de los resolutores de optimizacion de proposito general.
			-MULTICLASS PROBLEM:
				-basado en la regresion isotonica multiclase
				-la idea es descomponer el problema de K-clases en varios problemas binarios
				 y aplicar regresion isotonica a cada uno de los problemas.
				-Esta demostrado que la descomposicion del problema de estimacion de 
				 probabilidad para el caso de multiclase  siempre forman una adecuada 
				 distribucion de probabilidad, es decir, que siempre son no-negativos
				 y la suma es igual a 1.

		-APROXIMACION DIRECTA
			-consideramos la clasificacion directa basada en la minimizacion del riesgo 
			 empirico dentro de la clase de todas las funciones monotonas.
			-aunque este tipo de funciones no se pueden describir con un numero 
			 finito de parametros, la minimizacion del riesgo puede realizarse debido a 
			 que solo estamos interesados en valores de funciones monotonas en ciertos
			 puntos, los incluidos en D (training set).
			-Una funcion monotona minimizando el riesgo empirico puede obtenerse resolviendo
			 el siguiente problema de optimizacion:
			 	minimizar: SUM(L(yi,di))
			 	sujeto a las restricciones de monotonia.
			 	-donde di son variables del problema (valores de la funcion monotona optima
			 	 en puntos de D)
			-el problema puede tener otra interpretacion interesante: reetiquetar las
			 instancias para hacer el dataset monotono de forma que las etiquetas de las
			 instancias sean lo mas parecidas a las del conjunto original, donde esta 
			 similitud es medida en terminos de la funcion de perdida. Estas nuevas
			 etiquetas seran los nuevos valores optimos de las variables di. 
			 Este reetiquetado puede realizarse en el proceso de preprocesamiento
			 y corresponde a la correccion del error no parametrico.
			-como el problema de clasificacion no parametrica se asimila al de la regresion
			 isotonica (exceptuando que ahora se considera una salida discreta), será llamado
			 ahora "clasificacion isotonica" y su solucion optima d^ será llamada
			 "clasificacion optima de y"

___________________________________________________________________________________________________
	
	-1811.07155: monotonic classification: an overview on algorithms,
				 performance measures an data sets.

		-ejemplos de uso de monotonicidad:
			-comparacion de dos compañias donde una domina sobre la otra en terminos de
			 TODOS los indicadores financieros. Debido a esto, la compañia dominante
			 ha de tener una evaluacion final superior a la compañia dominada. Un uso
			 de esto, es la prediccion de la calificacion crediticia usada por los bancos.
			-House pricing: el precio de una casa sera superior cuantas mas habitaciones
			 posea, mejor sea la calidad del aire acondicionado y menor sea la polución
			 en el ambiente.

		-la consideracion de restricciones monotonicas queda motivada por dos hechos:
			1-el tamaño del espacio de la hipotesis es reducido, lo que facilita el
			  proceso de aprendizaje. (¿existencia de un numero reducido de clases?)
			2-otras metricas ademas de la precision, como la consistencia con respecto
			  a estas restricciones, pueden ser usadas por los expertos para aceptar
			  o rechazar el modelo. Estas técnicas de evaluacion de restricciones
			  monotónicas las veremos mas adelante con el fin de poder evaluar 
			  la consistencia de estas.

		-DEFINICION DE CLASIFICACION MONTONICA:
			-una restriccion monotonica siempre involucra  un atributo de entrada y el
			 atributo de clase, y ha de haber, al menos, una restriccion de monotonicidad
			 para distinguir entre clasificacion ordinal y monotonica.
			-la clasificacion monotonica puede ser directa (más habitaciones, precio mayor
			 de una casa), o inversa (más polucion, precio menor de la casa).
			-Normalmente en problemas de clasificacion monotonica reales, las restricciones
			 monotonicas son consideradas en un subconjunto de caracteristicas del dataset.
			-DESCRIPCION FORMAL DEL PROBLEMA -> muy bien explicada en el paper(apartado 2,
			 pagina 6)

		-TAXONOMIA PARA ALGORITMOS DE CLASIFICACION MONOTONICA
			-categorizacion basada en el proposito de cada metodo, la heuristica que
			 sigue y el modelo generado por cada algoritmo. Division de algoritmos:
			 	1-Clasificadores monotonicos: cuyo objetivo es crear modelos predictivos
			 	  que satisfagan las restricciones monotonicas de forma parcial o total.
			 	  Clasificadores en funcion del modelo que construyen:
			 	  		-clasificadores basados en instancias: no crean un modelo, directamente
			 	  		 utilizan las instancias del data set para la realizacion de decisiones
			 	  		-arboles de decision o reglas de clasificacion: los modelos creados
			 	  		 involucran reglas de produccion legibles en forma de arboles de
			 	  		 decision o conjunto de reglas.
			 	  		-Ensembles o multiclasificadores: uso de varios clasificadores para
			 	  		 obtener diferentes respuestas y posteriormente agregarlas en una
			 	  		 decision de clasificacion global:
			 	  		 	-boosting
			 	  		 	-bagging
			 	  		-redes neuronales: modelo bioinspirado en el que la funcion que
			 	  		 relaciona las entradas con la salida son bloques de construccion
			 	  		 (neuronas) interconectadas entre si y organizadas en capas.
			 	  		 Mediante un proceso de entrenamiento interativo se alcanzan
			 	  		 los pesos adecuados para cada conexion.
			 	  		-support vector machine: aprendizaje basado en maquinas de soporte
			 	  		 vectorial y derivadas
			 	  		-hibrido: combinacion de varios algoritmos (rules + instance based
			 	  		 learning i.e.)
			 	  		-fuzzy integral: basados en el uso de la integral de Choquet (para
			 	  		 medir la utilidad esperada de un evento incierto). Puede ser vista
			 	  		 como una generalizacion de la integral estandar de Lebesque para
			 	  		 el caso de medidas no aditivas.
			 	2-Preprocesamiento monotonico: refina los conjuntos de datos para mejorar el
			 	  rendimiento de los clasificadores monotonicos.
			 	  		-relabeling: para minimizar el numero de violaciones de monotonicidad
			 	  		 presentes en el data set. (casos en los que la variable de salida
			 	  		 no concuerda con las restricciones de monotonicidad)
			 	  		-feature selection: obtener los atributos mas relevantes para mejorar
			 	  		 el rendimiento de la clasificacion monotonica
			 	  		-instance selection: seleccion de subconjunto de instancias del 
			 	  		 data set con el objetivo de derivar en mejores clasificadores
			 	  		 monotonicos
			 	  		-seleccion de conjunto de entrenamiento: la heuristica seguida por
			 	  		 estos algoritmos ha de ser generica de forma que el subconjunto
			 	  		 seleccionado sea el que mejores resultados produce sin importar el 
			 	  		 clasificador utilizado posteriormente.
			 	 
			-Ejemplos de algoritmos basados en arboles:
				-MID(Monotonic induction of decision trees):
					-uso del arbol de clasificacion ID3.
					-Ben-David introduce una medida de monotonicidad denotada como
					 marcador-totalmente-ambiguo
					-para calcularla se construye una matriz bxb no monotonica, concerniente
					 a un arbol que contiene b ramas.
					-cada valor mij es 1 si las ramas i y j son no monotonas y 0 si lo son.

				-Positive/Quasi-Positive Decision Tree:
					-la regla de separacion consiste en separar los puntos que tienen
					 el hijo derecho mayor que el izquierdo (en terminos de la variable
					 de salida)
					-el algoritmo añade muestras a los nodos de forma que el arbol resultante
					 es monotono.
					-se requiere aplicacion sobre data sets estrictamente monotonos y binarios
					 (que contengan solo dos clases)

				-Monotonic Decision Tree:
					-pensado para generar arboles de decision motononicos a partir de conjuntos
					 de muestras que pueden no se monotonicos o consistentes.
					-construye el arbol usando un conjunto de etiquetas ordinales que no son
					 las mismas que las originales.
					-puede usarse un proceso de mapeo para reetiquetarlas a las originales

				-Isotonic classification tree:
					-ajusta la probabilidad estimada en los nodos hoja en caso de una violacion
					 de la monotonicidad
					-idea: considerando la restriccion de monotonicidad, la suma de la 
					 prediccion absoluta de errores en la muestra de entrenamiento ha de ser
					 minimizada
					-ademas, el algoritmo puede soportar problemas donde algunos, pero no todos
					 los atributos, tienen una relacion monotonica con respecto a la variable
					 dependiente