Literatura generica de monotonicos:
	-Kotlowski:
		-clasificacion ordinal con restricciones monotonicas: maneja conocimiento subyacente del
		 problema sobre clases ordenadas, atributos ordenados y una relacion monotonica entre
		 la evaluacion de los atributos de una instancia y la asignacion de esta a una clase.
		-principio de dominancia: a mayor valor en atributos de una instancia, mayor sera el
		 valor de la clase a la que se asigna dicha instancia (clasificacion ordinal con
		 restricciones monotonicas)
		-si no hay relacion de monotonia en la asociacion de una clase a una instancia, 
		 pero las clases si poseen un orden se considera clasificacion ordinal tan solo
		-con las restricciones de monotonia presentes se puede trabajar con una amplia
		 variedad de funciones sin temor a que introduzcan más restricciones que 
		 la de monotonia: es posible hacer inferencia de la clase sobre todas las funciones
		 monotonas.
		-proposito del paper:
			-formalizar la aproximacion de clasificcacion con restricciones monotonicas
			-analizar los metodos no parametricos de clasificacion
				-plug-in: estimando la distribucion condicional de la clase. Provienen de la
				 clasificacion isotonica (monotona creciente o decreciente)
				-empirical risk (direct approach): minimizando el riesgo empirico
		-usando el termino de "relacion de dominancia" decimos que una instancia x domina a
		 otra instancia x' cuando cada una de las variables de entrada de x (atributos de x)
		 son mayores o iguales que cada uno de los de x', se denota x>=x' y por tanto
		 x tendra asignada una etiqueta de clase mayor que x'.
		-una funcion es monotona si x>=x' -> h(x)>=h(x'). Es decir, si x domina a x', la
		 inferencia de clase de x ha de ser superior a la de x'.
		-RESTRICCIONES MONOTONICAS:
			-seran restricciones con respecto a la probabilidad de distribución de los 
			 datos y con repecto a las imposiciones de la función de perdida bajo las cuales
			 el clasificador optimo de Bayes es monotono.
			-DOMINANCIA ESTOCASTICA:
				-la restriccion de monotonia comentada (si  x>=x' -> h(x)>=h(x')) no siempre
				 se aplica en la practica de forma tan restrictiva, por ello hemos de 
				 hablar en términos probabilisticos a la hora de referirnos a dichas
				 restricciones
				-Decimos entonces que, siendo 'k' una de las posibles clases a tomar en el 
				 dominio por una instancia 'x', y siendo 'y' la etiqueta asignada a una
				 instancia 'x', si la restricción monótona nos dice que x>=x', entonces
				 la dominancia estocastica nos dice que P(y<=k|x)<=P(y<=k|x').
				 Es decir, que la probabilidad de que el valor asignado (y) a la instancia 
				 dominante (x) sea mayor que cierto valor de la clase fijado (k), es mayor
				 que la probabilidad de que el valor asignado (y) a la instancia dominada(x')
				 sea mayor que cierto valor de la clase fijado.
				-la relacion de dominancia estocastica entre distribuciones se denota asi:
					x >= x' -> P(y|x) >= P(y|x')
			-CLASIFICADOR MONOTONO DE BAYES
				-en el problema de clasificacion el objetivo es encontrar el clasificador más
				 parecido al clasificador de Bayes, es decir, esta es nuestra función objetivo.
				-Siendo esta nuestra funcion objetivo, es un requisito que tambien aplique
				 las restricciones de monotonía que hemos enunciado. Problema: aunque la
				 distribucion de probabilidad tiene restricciones monotonicas, el clasificador
				 de Bayes no siempre las mantiene.
				-para solucionar este problema y mantener la monotonia en el clasificador de 
				 Bayes, han de imponerse las siguientes restricciones a la funcion de perdida
				 (L):
				 	-L(y,k+1)-L(y,k) >= L(y+1,k+1)-L(y+1,k)
					 	-demostrado en  Computer Society Digital Library at 
					 	http://doi.ieeecomputersociety.org/10.1109/TKDE.2012.204
					 	-esta caracteristica de la funcion de perdida es necesaria
					 	 en la clasificacion con restricciones monotonicas, si no no
					 	  tendria sentido minimizar el riesgo dentro de la clase de las funciones monotonas.
					-la siguiente definicion de convexidad es necesaria tambien para
					 mantener la restriccion de monotonia en el clasificador de Bayes:
					 	-siendo L(y,k) = c(y-k) (con c(0)=0)
					 	-la funcion c(k) es convexa si, para todo k entre -(k-1) y (k-1):
					 		c(k) <= (c(k-1)+c(k+1))/2
					 	-el clasificador de Bayes es monotono si y solo si c(k) (que es
					 	 la V-shaped loss function) es convexa.

		-APROXIMACIÓN PLUG-IN:
			-los metodos no parametricos son asi llamados porque explotan la clase de todas las
			 funciones monotonas. Estos son los que consideraremos a partir de ahora en el
			 paper. Estos metodos no hacen ninguna asuncion mas sobre el modelo que la de 
			 las restricciones monotonicas.
			-hemos de construir un metodo para estimar P(y|x), sabiendo que P(y|x) posee
			 dos ventajas:
			 	1-la distribucion condicional permite la determinacion de la prediccion 
			 	  optima para cualquier funcion de perdida.
			 	2-la distribucion condicional mide la confianza de la prediccion
			-el metodo de estimacion esta basado en la regresion isotonica.
			-PROBLEMA DE CLASIFICACION BINARIA Y REGRESION ISOTONICA:
				-en la aproximacion plugin se propone usar un vector de estimadores de densidad
				 condicional p = (p1,....,pn), el cual es una regresion isotonica del vector de
				 etiquetas y = (y1,...,yn). Es decir, p nos da la probabilidad de que x 
				 pertenezca a cada una de las clases existentes en y.
				-p es la solucion del problema: SUM((yi-pi)^2) sujeto a las restricciones de
				 monotonicidad (Xi>=Xj --> pi>=pj). Por ello p minimiza el error cuadratico en 
				 el conjunto de los vectores monotonos p=(p1,..,pn) para cada x.
				-la eleccion de la funcion de error (funcion de perdida de error cuadratico)
				 parece ser arbitraria. Puede verse que haciendo uso de otras funciones
				 de perdida, se llega al mismo resultado.