Literatura generica de monotonicos:
	-Kotlowski:
		-funcion monotona: una funcion es monotona si y solo si x<=y implica f(x)<=f(y)
		-clasificacion ordinal con restricciones monotonicas: maneja conocimiento subyacente del
		 problema sobre clases ordenadas, atributos ordenados y una relacion monotonica entre
		 la evaluacion de los atributos de una instancia y la asignacion de esta a una clase.
		-principio de dominancia: a mayor valor en atributos de una instancia, mayor sera el
		 valor de la clase a la que se asigna dicha instancia (clasificacion ordinal con
		 restricciones monotonicas)
		-si no hay relacion de monotonia en la asociacion de una clase a una instancia, 
		 pero las clases si poseen un orden se considera clasificacion ordinal tan solo
		-con las restricciones de monotonia presentes se puede trabajar con una amplia
		 variedad de funciones sin temor a que introduzcan más restricciones que 
		 la de monotonia: es posible hacer inferencia de la clase sobre todas las funciones
		 monotonas.
		-proposito del paper:
			-formalizar la aproximacion de clasificacion con restricciones monotonicas
			-analizar los metodos no parametricos de clasificacion
				-plug-in: estimando la distribucion condicional de la clase. Provienen de la
				 clasificacion isotonica (monotona creciente o decreciente)
				-empirical risk (direct approach): minimizando el riesgo empirico
		-usando el termino de "relacion de dominancia" decimos que una instancia x domina a
		 otra instancia x' cuando cada una de las variables de entrada de x (atributos de x)
		 son mayores o iguales que cada uno de los de x', se denota x>=x' y por tanto
		 x tendra asignada una etiqueta de clase mayor que x'.
		-una funcion es monotona si x>=x' -> h(x)>=h(x'). Es decir, si x domina a x', la
		 inferencia de clase de x ha de ser superior a la de x'.
		-RESTRICCIONES MONOTONICAS:
			-seran restricciones con respecto a la probabilidad de distribución de los 
			 datos y con repecto a las imposiciones de la función de perdida bajo las cuales
			 el clasificador optimo de Bayes es monotono.
			-DOMINANCIA ESTOCASTICA:
				-la restriccion de monotonia comentada (si  x>=x' -> h(x)>=h(x')) no siempre
				 se aplica en la practica de forma tan restrictiva, por ello hemos de 
				 hablar en términos probabilisticos a la hora de referirnos a dichas
				 restricciones
				-Decimos entonces que, siendo 'k' una de las posibles clases a tomar en el 
				 dominio por una instancia 'x', y siendo 'y' la etiqueta asignada a una
				 instancia 'x', si la restricción monótona nos dice que x>=x', entonces
				 la dominancia estocastica nos dice que P(y<=k|x)<=P(y<=k|x').
				 Es decir, que la probabilidad de que el valor asignado (y) a la instancia 
				 dominante (x) sea mayor que cierto valor de la clase fijado (k), es mayor
				 que la probabilidad de que el valor asignado (y) a la instancia dominada(x')
				 sea mayor que ese mismo cierto valor de la clase fijado.
				-la relacion de dominancia estocastica entre distribuciones se denota asi:
					x >= x' -> P(y|x) >= P(y|x')
			-CLASIFICADOR MONOTONO DE BAYES
				-en el problema de clasificacion el objetivo es encontrar el clasificador más
				 parecido al clasificador de Bayes, es decir, esta es nuestra función objetivo.
				-Siendo esta nuestra funcion objetivo, es un requisito que tambien aplique
				 las restricciones de monotonía que hemos enunciado. Problema: aunque la
				 distribucion de probabilidad tiene restricciones monotonicas, el clasificador
				 de Bayes no siempre las mantiene.
				-para solucionar este problema y mantener la monotonia en el clasificador de 
				 Bayes, han de imponerse las siguientes restricciones a la funcion de perdida
				 (L):
				 	-L(y,k+1)-L(y,k) >= L(y+1,k+1)-L(y+1,k)
					 	-demostrado en  Computer Society Digital Library at 
					 	http://doi.ieeecomputersociety.org/10.1109/TKDE.2012.204
					 	-esta caracteristica de la funcion de perdida es necesaria
					 	 en la clasificacion con restricciones monotonicas, si no no
					 	  tendria sentido minimizar el riesgo dentro de la clase de las funciones monotonas.
					-la siguiente definicion de convexidad es necesaria tambien para
					 mantener la restriccion de monotonia en el clasificador de Bayes:
					 	-siendo L(y,k) = c(y-k) (con c(0)=0)
					 	-la funcion c(k) es convexa si, para todo k entre -(k-1) y (k-1):
					 		c(k) <= (c(k-1)+c(k+1))/2
					 	-el clasificador de Bayes es monotono si y solo si c(k) (que es
					 	 la V-shaped loss function) es convexa.

		-APROXIMACIÓN PLUG-IN:
			-los metodos no parametricos son asi llamados porque explotan la clase de todas las
			 funciones monotonas. Estos son los que consideraremos a partir de ahora en el
			 paper. Estos metodos no hacen ninguna asuncion mas sobre el modelo que la de 
			 las restricciones monotonicas.
			-hemos de construir un metodo para estimar P(y|x), sabiendo que P(y|x) posee
			 dos ventajas:
			 	1-la distribucion condicional permite la determinacion de la prediccion 
			 	  optima para cualquier funcion de perdida.
			 	2-la distribucion condicional mide la confianza de la prediccion
			-el metodo de estimacion esta basado en la regresion isotonica.
			-PROBLEMA DE CLASIFICACION BINARIA Y REGRESION ISOTONICA:
				-en la aproximacion plugin se propone usar un vector de estimadores de densidad
				 condicional p = (p1,....,pn), el cual es una regresion isotonica del vector de
				 etiquetas y = (y1,...,yn). Es decir, p nos da la probabilidad de que x 
				 pertenezca a cada una de las clases existentes en y.
				-p es la solucion del problema: SUM((yi-pi)^2) sujeto a las restricciones de
				 monotonicidad (Xi>=Xj --> pi>=pj). Por ello p minimiza el error cuadratico en 
				 el conjunto de los vectores monotonos p=(p1,..,pn) para cada x.
				-la eleccion de la funcion de error (funcion de perdida de error cuadratico)
				 parece ser arbitraria. Puede verse que haciendo uso de otras funciones
				 de perdida, se llega al mismo resultado.
				-la regresion isotonica es un problema de optimizacion cuadratica con 
				 restricciones lineales, por ello puede ser resuelta de forma eficiente con
				 la mayoria de los resolutores de optimizacion de proposito general.
			-MULTICLASS PROBLEM:
				-basado en la regresion isotonica multiclase
				-la idea es descomponer el problema de K-clases en varios problemas binarios
				 y aplicar regresion isotonica a cada uno de los problemas.
				-Esta demostrado que la descomposicion del problema de estimacion de 
				 probabilidad para el caso de multiclase  siempre forman una adecuada 
				 distribucion de probabilidad, es decir, que siempre son no-negativos
				 y la suma es igual a 1.

		-APROXIMACION DIRECTA
			-consideramos la clasificacion directa basada en la minimizacion del riesgo 
			 empirico dentro de la clase de todas las funciones monotonas.
			-aunque este tipo de funciones no se pueden describir con un numero 
			 finito de parametros, la minimizacion del riesgo puede realizarse debido a 
			 que solo estamos interesados en valores de funciones monotonas en ciertos
			 puntos, los incluidos en D (training set).
			-Una funcion monotona minimizando el riesgo empirico puede obtenerse resolviendo
			 el siguiente problema de optimizacion:
			 	minimizar: SUM(L(yi,di))
			 	sujeto a las restricciones de monotonia.
			 	-donde di son variables del problema (valores de la funcion monotona optima
			 	 en puntos de D)
			-el problema puede tener otra interpretacion interesante: reetiquetar las
			 instancias para hacer el dataset monotono de forma que las etiquetas de las
			 instancias sean lo mas parecidas a las del conjunto original, donde esta 
			 similitud es medida en terminos de la funcion de perdida. Estas nuevas
			 etiquetas seran los nuevos valores optimos de las variables di. 
			 Este reetiquetado puede realizarse en el proceso de preprocesamiento
			 y corresponde a la correccion del error no parametrico.
			-como el problema de clasificacion no parametrica se asimila al de la regresion
			 isotonica (exceptuando que ahora se considera una salida discreta), será llamado
			 ahora "clasificacion isotonica" y su solucion optima d^ será llamada
			 "clasificacion optima de y"

___________________________________________________________________________________________________
	
	-1811.07155: monotonic classification: an overview on algorithms,
				 performance measures an data sets.

		-ejemplos de uso de monotonicidad:
			-comparacion de dos compañias donde una domina sobre la otra en terminos de
			 TODOS los indicadores financieros. Debido a esto, la compañia dominante
			 ha de tener una evaluacion final superior a la compañia dominada. Un uso
			 de esto, es la prediccion de la calificacion crediticia usada por los bancos.
			-House pricing: el precio de una casa sera superior cuantas mas habitaciones
			 posea, mejor sea la calidad del aire acondicionado y menor sea la polución
			 en el ambiente.

		-la consideracion de restricciones monotonicas queda motivada por dos hechos:
			1-el tamaño del espacio de la hipotesis es reducido, lo que facilita el
			  proceso de aprendizaje. (¿existencia de un numero reducido de clases?)
			2-otras metricas ademas de la precision, como la consistencia con respecto
			  a estas restricciones, pueden ser usadas por los expertos para aceptar
			  o rechazar el modelo. Estas técnicas de evaluacion de restricciones
			  monotónicas las veremos mas adelante con el fin de poder evaluar 
			  la consistencia de estas.

		-DEFINICION DE CLASIFICACION MONTONICA:
			-una restriccion monotonica siempre involucra  un atributo de entrada y el
			 atributo de clase, y ha de haber, al menos, una restriccion de monotonicidad
			 para distinguir entre clasificacion ordinal y monotonica.
			-la clasificacion monotonica puede ser directa (más habitaciones, precio mayor
			 de una casa), o inversa (más polucion, precio menor de la casa).
			-Normalmente en problemas de clasificacion monotonica reales, las restricciones
			 monotonicas son consideradas en un subconjunto de caracteristicas del dataset.
			-DESCRIPCION FORMAL DEL PROBLEMA -> muy bien explicada en el paper(apartado 2,
			 pagina 6)

		-TAXONOMIA PARA ALGORITMOS DE CLASIFICACION MONOTONICA
			-categorizacion basada en el proposito de cada metodo, la heuristica que
			 sigue y el modelo generado por cada algoritmo. Division de algoritmos:
			 	1-Clasificadores monotonicos: cuyo objetivo es crear modelos predictivos
			 	  que satisfagan las restricciones monotonicas de forma parcial o total.
			 	  Clasificadores en funcion del modelo que construyen:
			 	  		-clasificadores basados en instancias: no crean un modelo, directamente
			 	  		 utilizan las instancias del data set para la realizacion de decisiones
			 	  		-arboles de decision o reglas de clasificacion: los modelos creados
			 	  		 involucran reglas de produccion legibles en forma de arboles de
			 	  		 decision o conjunto de reglas.
			 	  		-Ensembles o multiclasificadores: uso de varios clasificadores para
			 	  		 obtener diferentes respuestas y posteriormente agregarlas en una
			 	  		 decision de clasificacion global:
			 	  		 	-boosting
			 	  		 	-bagging
			 	  		-redes neuronales: modelo bioinspirado en el que la funcion que
			 	  		 relaciona las entradas con la salida son bloques de construccion
			 	  		 (neuronas) interconectadas entre si y organizadas en capas.
			 	  		 Mediante un proceso de entrenamiento interativo se alcanzan
			 	  		 los pesos adecuados para cada conexion.
			 	  		-support vector machine: aprendizaje basado en maquinas de soporte
			 	  		 vectorial y derivadas
			 	  		-hibrido: combinacion de varios algoritmos (rules + instance based
			 	  		 learning i.e.)
			 	  		-fuzzy integral: basados en el uso de la integral de Choquet (para
			 	  		 medir la utilidad esperada de un evento incierto). Puede ser vista
			 	  		 como una generalizacion de la integral estandar de Lebesque para
			 	  		 el caso de medidas no aditivas.
			 	2-Preprocesamiento monotonico: refina los conjuntos de datos para mejorar el
			 	  rendimiento de los clasificadores monotonicos.
			 	  		-relabeling: para minimizar el numero de violaciones de monotonicidad
			 	  		 presentes en el data set. (casos en los que la variable de salida
			 	  		 no concuerda con las restricciones de monotonicidad)
			 	  		-feature selection: obtener los atributos mas relevantes para mejorar
			 	  		 el rendimiento de la clasificacion monotonica
			 	  		-instance selection: seleccion de subconjunto de instancias del 
			 	  		 data set con el objetivo de derivar en mejores clasificadores
			 	  		 monotonicos
			 	  		-seleccion de conjunto de entrenamiento: la heuristica seguida por
			 	  		 estos algoritmos ha de ser generica de forma que el subconjunto
			 	  		 seleccionado sea el que mejores resultados produce sin importar el 
			 	  		 clasificador utilizado posteriormente.
			 	 
			-Ejemplos de algoritmos basados en arboles:
				-MID(Monotonic induction of decision trees):
					-uso del arbol de clasificacion ID3.
					-Ben-David introduce una medida de monotonicidad denotada como
					 marcador-totalmente-ambiguo
					-para calcularla se construye una matriz bxb no monotonica, concerniente
					 a un arbol que contiene b ramas.
					-cada valor mij es 1 si las ramas i y j son no monotonas y 0 si lo son.

				-Positive/Quasi-Positive Decision Tree:
					-la regla de separacion consiste en separar los puntos que tienen
					 el hijo derecho mayor que el izquierdo (en terminos de la variable
					 de salida)
					-el algoritmo añade muestras a los nodos de forma que el arbol resultante
					 es monotono.
					-se requiere aplicacion sobre data sets estrictamente monotonos y binarios
					 (que contengan solo dos clases)

				-Monotonic Decision Tree:
					-pensado para generar arboles de decision motononicos a partir de conjuntos
					 de muestras que pueden no se monotonicos o consistentes.
					-construye el arbol usando un conjunto de etiquetas ordinales que no son
					 las mismas que las originales.
					-puede usarse un proceso de mapeo para reetiquetarlas a las originales

				-Isotonic classification tree:
					-ajusta la probabilidad estimada en los nodos hoja en caso de una violacion
					 de la monotonicidad
					-idea: considerando la restriccion de monotonicidad, la suma de la 
					 prediccion absoluta de errores en la muestra de entrenamiento ha de ser
					 minimizada
					-ademas, el algoritmo puede soportar problemas donde algunos, pero no todos
					 los atributos, tienen una relacion monotonica con respecto a la variable
					 dependiente

			-preprocesamiento monotonico:
				-Relabeling: cambio de etiqueta para las instancias que producen violacion
							 de las restricciones monotonicas que producen conjuntos de
							 datos completamente monotonos, los cuales son necesarios
							 para muchos clasificadores monotonicos.
						-Dykstra Relabel: relabeling basado en la regresión monotonica, 
										  capaz de minimiazr el error absoluto o el 
										  error cuadratico.
										  Es optimo con estas loss functions pero no
										  garantiza el numero minimo de cambio de 
										  etiquetas debido a que no es el objetivo.
						-Daniels-Velikova Greedy Relabel: algoritmo greedy para reetiquetar
										  ejemplos no monotonos uno por uno. En cada iteracion
										  busca una instancia y la nueva etiqueta para
										  maximizar la monotonicidad del data set.
										  Aunque en cada iteracion es capaz de maximizar el
										  salto hacia la completa monotonicidad, el algoritmo
										  reetiqueta mas ejemplos de los necesarios.
										  No garantiza solucion optima.
						-Optimal Flow Network Relabel: basado en encontrar un conjunto 
										  independiente de pesos maximo en el grafo de 
										  violacion de monotonicidad. Reetiquetando el
										  complemento de este conjunto, resulta en un 
										  data set monotono con los menores cambios posibles.
										  Este metodo es optimo, produciendo el menor numero
										  de cambios posibles. (Optimo?)
						-Single-pass Optimal Ordinal Relabel: la idea es explorat las 
										  propiedades de una red de flujo minima e identificar
										  propiedades agradables de algunos cortes maximos.
										  Es un algoritmo optimo de reetiquetado.
				-Feature Selection: mejorar capacidad predictiva de clasificadores monotonicos
									seleccionando las caracteristicas mas relevantes.
						-O-ReliefF/O-Simba: Los autores introducen algoritmos de selección de
							 características basados en márgenes para la clasificación monotónica al incorporar las restricciones de monotonicidad en la
							 tarea ordinal. Los métodos ReliefF y Simba se extienden al
							 contexto de la clasificación ordinal.
						-min-Redundancy max-Relevance(mRMR): integra el ranking de la metrica
							de informacion mutua con la estrategia de busqueda de la minima
							redundancia y la maxima relevancia, creando un algoritmo efectivo
							para el objetivo propuesto.
						-Non-monotonic feature selection via Multiple Kernel Learning:	
							seleccion de caracteristicas no monotonica con alivio de violacion
							de restricciones monotonicas mediante la computacion de socores 
							para cada caracteristica individual que depende de la cantidad
							de caracteristicas selccionadas.
				-Instance Selecion: mediante el uso de heuristicas basadas en instancias, se
									pretende mejorar el rendimiento de los clasificadores
									monotonicos escogiendo aquellas instancias más relevantes
									para el caso de estudio.
						-Monotonic Iterative Prototype Selection (MONIPS):
							Sigue un esquema iterativo en el que determina las instancias más
							representativas que mantienen o mejoran las capacidades predictivas
							del algoritmo MkNN. Realiza un borrado de instancias basado en la
							mejora del rendimiento de MkNN.
				-Training Set Selection: estos algoritmos poseen el mismo objetivo que los 
										anteriores, con la única diferencia de que la 
										heuristica seguida ha de ser generica de tal forma que 
										el subconjunto seleccionado ha de ser el que 
										proporcione el mejor rendimiento independientemente del
										clasificador que se utilice más tarde sobre él.
						-Monotonic Training Set Selection(MonTSS):
							Incorpora las medidas necesarias para identificar y seleccionar
							las instancias apropiadas en el conjunto de datos para mejorar
							tanto el accuracy como la naturaleza monotonica de los modelos
							producidos por diferentes clasificadores.

		-QUALITY METRICS USED IN MONOTONIC CLASSIFICATION (PAGINA 23)
			-metricas de evaluacion predictiva:
				-Clasificacion binaria:
					-accuracy: representa la habilidad predictiva de acuerdo con la proporción de 
							   los datos clasificados de forma correcta.
					-error rate: caso opuesto al accuracy, evaluando la proporcion de los datos
								 evaluados clasificados de forma incorrecta.
					-recall/sensitivity: medida de la proporcion de TP que estan correctamente clasificados
					-false positive rate: FP/FP+TN
					-positive predictive value(PPV) / precision: proporcion de las instancias del test que tienen una
													 salida positiva y que ademas están bien clasificados.
													 representa la probabilidad de que una prueba positiva 
													 refleje la condición subyacente que se está probando.
					-negative predictive value(NPV): proporcion de instancias de test con valores de prediccion
													 negativos que estan clasificados de forma correcta.
					-F-Measure: es la media armónica (inverso de media aritmética) de las medidas precision y recall.
					-Arean Under Curve(AUC): para combiniar Recall y false positive rate en una sola medida, primeramente
											 calculamos cada una de estas medidas con varios thresold distintos para la
											 regresión logística y después los ploteamos en una sola gráfica con 
											 el ratio de falsos positivos en las abcisas y el Recall en el eje de
											 ordenadas. El resultado es la curva ROC, y la métrica que consideramos es 
											 el área bajo esta curva.
				-Clasificación multiclase:
					-Mean Squared Error: mide la media de los cuadrados de los errores
					-Mean Absolute Error: mide como de cerca se encuentran las predicciones de los valores reales de salida (igual que MSE pero sin el cuadrado)
					-Monotonic Accuracy: igual que accuracy pero tan solo usando las instancias que cumplen los requisitos de monotonia en el conjunto de test.
					-Monotonic Mean Absolute Error: igual que MAE, pero considerando solo las instancias que cumplen los requisitos de monotonia en el conjunto de test.

			-metricas de cumplimiento de monotonicidad: el interés consiste en evaluar el ratio de monotonicidad provisto por las 
														predicciones obtenidas o por el modelo construido.
														NClash(x) = numero de instancias del data set que NO cumplen las restricciones de monotonia con respecto a la instancia x
														NMonot(x) = numero de instancias del data set que SI cumplen las restricciones de monotonia con respecto a la instancia x

							-el indice de no monotonicidad (NMI) es el numero de parejas de choque (parejas que no cumplen las restricciones monotonicas) dividido por el numero
							 total de parejas en el data set.
							-frecuencia de monotonicidad (FOM) es la suma de las parejas que cumplen las restricciones de moonotonicidad dividido entre el número total de parejas.

		MEDIDAS CON PROPOSITOS PREDICTIVOS MAS USADAS:
			-MAE
			-NMI

_______________________________________________________________________________________________________________________________________________________________________________________
_______________________________________________________________________________________________________________________________________________________________________________________
_______________________________________________________________________________________________________________________________________________________________________________________

Literatura generica de data streams:
	-Gama_2012_article.....:A survey on learning from data streams: current and future trends
		-Los sistemas tradicionales basados en el uso de memoria, entrenados de una forma fija mediante conjuntos de entrenamiento y los cuales generan modelos estáticos no están
		 preparados para procesar los datos altamente detallados disponibles en procesos como, por ejemplo, el continuo análisis del consumo de energía de una red electrica en distintas zonas
		 en el espacio, lo cual genera una gran cantidad de datos que ha de ser procesada de forma rápida con el fin de generar modelos predictivos consistentes
		 que se adapten a situaciones cambiantes y puedan reaccionar de forma rápida y eficaz a dichos cambios.

		 -MACHINE LEARNING DATA STREAMS:
		 	-el Machine Learning extrae conocimiento en forma de modelos y patrones de unos datos de naturaleza cambiante. Hoy en día la generación de datos, gracias a las capacidades
		 	 tecnológicas de las que disfrutamos, se produce a altas velocidades, tanto así, que se pone, en cuanto a velocidad, por delante del procesamiento de dichos datos, lo cual
		 	 quiere decir que generamos datos a mayor velocidad de lo que las capacidades computacionales que tenemos ahora mismo nos permiten procesarlos.
		 	 Desde este punto de vista, conviene modelar los datos como flujos de datos transitorios en lugar de como tablas de datos persistentes.

		 	 HACER CAPTURA DE PANTALLA DE LA TABLA 1 EN LA PAGINA 2 del PDF

		 	 -Streaming algorithms:
		 	 	-2 modelos distintos:
		 	 		-insert-only model: entran datos al sistema de forma secuencial.
		 	 		-insert-delete model: los elementos que entran pueden ser eliminados o actualizados.
		 	 	-desde el punto de vista de los sistemas de control de flujo de datos(DSMS), existen varios problemas que requieren técnicas de procesamiento aproximadas para evaluar el flujo 
		 	 	 continuo de datos que requieren una cantidad ilimitada de memoria.
		 	 	-los algoritmos que procesan flujos de datos producen soluciones aproximadas dentro de un rango de error con una alta probabilidad, relajando así las restricciones a la hora
		 	 	 de obtener una solución exacta.
		 	 	-en algunas aplicaciones una respuesta aproximada deberia estar dentro de un rango de error admisible. Los sistemas de control de flujos de datos han desarrollado un conjunto
		 	 	 de técnicas que almacenan resumenes de datos compactos suficientes para resolver consultas. Estas aproximaciones requieren un equilibrio entre el accuracy y la cantidad de
		 	 	 memoria usada para almacenar los resumenes, con una restricción adicional de tiempo de procesado de los datos.
		 	 -Aproximacion y aleatorización
		 	 	Dentro del marco del data streaming, como ya hemos dicho, está permitido ofrecer respuestas aproximadas dentro de un pequeño rango de error (e), con una pequeña probabilidad
		 	 	de fallo (d) para obtener respuestas con una probabilidad de que 1-d se encuentre en el intervalo de radio e.
		 	 	Los algoritmos que usan dichas aproximacióon y aleatorización son referidos por dichos (e,d).
		 	 	la idea basicamente consiste en mapear cada espacio grande de entrada en una sinopsis pequeña.
		 	 	La aproximación y randomización han sido usadas en solventar problemas como mineria de reglas de asociación, items frecuentes, k-means...
		 	 -Ventanas de tiempo	
		 	 	-Para la realización del cómputo estadístico referente al modelo de flujos, no nos interesa el total de los datos existentes, si no los más recientes, entendiendo que son los
		 	 	 que mejor explican la situación a la que nos enfrentamos y pudiendo, de esta forma, deshacernos de grandes cantidades de datos que no nos son útiles.
		 	 	 Las técnicas más simples en cuanto a este tipo de tratamiento de datos, utilizan una ventana deslizante de tamaño fijo, con un funcionmiento FIFO (first in fist out).
		 	 	 Definimos dos tipos de ventana deslizante:
		 	 	 	-basada en sequencia: tamaño de ventana definido por el número de observaciones (tamaño fijo o variable en el tiempo)
		 	 	 	-basada en marca de tiempo: tamaño de ventana definido en términos de duración. Una ventana de este tipo de tamaño t consiste en todos los elementos cuya marca de tiempo
		 	 	 								se situa dentro del intervalo de tiempo t del actual periodo de tiempo.
		 	 	 El hecho de monitorizar, analizar y extraer conocimiento de flujos de datos de alta velocidad, puede hacer que existan diversos niveles de granularidad a la hora de 
		 	 	 almacenar los datos. Conforme más antiguos son los datos que disponemos, mayor granularidad requeriremos en la información (es decir, menor precisión). Cuanto más
		 	 	 reciente sean los datos, el grano ha de ser más fino, ya que requerimos más precisión al tratarlos debido a que son más importantes (este es llamado el modelo de ventana de 
		 	 	 tiempo inclinado).	
		 	 	 AdWin-ADaptive sliding WINdow: mantiene una ventana variable con respecto a los items recientemente vistos con la propiedad de que la ventana tiene un tamaño maximal
		 	 	 estadísticamente consistente con la hipótesis de que no haya habido un cambio en la media del valor dentro de la ventana. Un fragmento viejo de la ventana se desecha
		 	 	 si hay alguna evidencia de que tiene un valor distinto al del resto de la ventana.
		 	 -sampling	
		 	 	-seleccion del subconjunto de datos a analizar en intervalos periodicos.
		 	 	-se usa para calcular estadisticas del flujo (valores esperados)
		 	 	-reducen coste cantidad de datos a procesar, por tanto, el computacional
		 	 	-pueden ser una fuente de errores, por ejemplo, en aplicaciones dedicadas a la detección de valores extremos o anomalias
		 	 	-el problema principal es obtener una muestra representativa
		 	 	-tecnicas:
		 	 		-random sampling
		 	 		-reservoir sampling
		 	 		-load shedding
		 -ALGORITHMS FOR LEARNING FROM DATA STREAMS






		 	 -sinopsis bocetos y resumenes
		 	 	-sinopsis: estructuras de datos compactas que resumen datos para su posterior consulta.
		 	 	-data sketching: herramienta de reducción de dimensionalidad. Usa proyecciones aleatorias de datos con cierta dimensión d a un espacio de cierto conjunto de dimensiones.
		 	 	-data stream summary (by Cormode and Muthukrishnan): usado para aproximaciones (e,d) para resolver consultas de rango, consultas puntuales y consultas innerproduct.